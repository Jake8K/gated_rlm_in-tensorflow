# Gated word-char RLM in TensorFlow

### A natural language processing deep learning model in tensorflow attempting to generate true Shakespearean sonnets with the proper rhyme and meter schemes. We hope that by adding an attention mechanism to [Myiamoto & Cho's Gated RLM](https://arxiv.org/abs/1606.01700), the model will be able to pick up on the semantics and morpheme level information to generate solid poems. Our thoughts are expanded in our [report](docs/Andromeda_Capstone_Report.pdf) and the [original publication](Gated_Word-Character_Recurrent_Language_Model.pdf) can be found in this repo as well. Their original source code, in Theano, can be found [here](https://github.com/nyu-dl/gated_word_char_rlm).

If anyone would like to collaborate on this, I would appreciate some help. I have no formal training in machine learning or language processing, this is tough stuff!

## To Tokenize Files:
```$ python tokenize_file.py <File Names>```

## To Train Model:
```(tensorflow)$ python train.py```
     * assumes default settings and file hierarchy, bellow, options yet to be set

## To Sample from Model:
```(tensorflow)$ python sample.py```
     * NOT YET IMPLEMENTED

### Folder Structure:
gated-rlm  (Gated Recurrent Language Model)
|
|--SourceCode
|    |-[base.py](docs/SourceCode/base.py)
|    |      (basic helper functions & Keras’ Progbar object to visualize training)
|    |-[data_preprocess.py](docs/SourceCode/data_preprocess.py)
|    |      (from Miyamoto and Cho’s source code, some functions were slightly modified
|    |       to accommodate our model and/or TensorFlow)
|    |-[layers.py](docs/SourceCode/layers.py)
|    |      (the layers for the model)
|    |-[gated_rlm.py](docs/SourceCode/gated_rlm.py)
|    |      (an implementation of the model using TensorFlows app.flags function to initialize,
|    |       share, and update variables through the TensorFlow session)
|    |-[train.py](docs/SourceCode/train.py)
|    |      (the training script for the gated rlm model,
|    |       though most of the training logic still resides in gated_rlm.py)
|    |-[wordChar_prep.py](docs/SourceCode/wordChar_prep.py)
|    |      (some basic utils)
|    |-[sample.py](docs/SourceCode/sample.py)
|    |      (the sampling script for the gated rlm model to sample output of trained models,
|    |       not yet implemented)
|    |--run_dir
|    |   |-files generated by the model (checkpoints, stats, temporary files, etc)
|    |      (recommended to run th model from here with ```(tensorflow)$python ../train.py``` command)
|    |
|    |--data
|        |--MnC_dicts
|        |   |-[char_dict.pkl](docs/SourceCode/data/MnC_dicts/char_dict.pkl)
|        |   |      (character vocab for the model from Miyamoto and Cho’s script)
|        |   |-[word_dict.pkl](docs/SourceCode/data/MnC_dicts/word_dict.pkl)
|        |          (word vocab for the model from Miyamoto and Cho’s script)
|        |
|        |--yoon_dicts
|        |   |-[char_vocab.pkl](docs/SourceCode/data/yoon_dicts/char_vocab.pkl)
|        |   |      (character vocab for the model from Yoon Kim’s script)
|        |   |-[word_vocab.pkl](docs/SourceCode/data/yoon_dicts/word_vocab.pkl)
|        |          (word vocab for the model from Yoon Kim’s script)
|        |
|        |--basic_dicts
|        |   |-[char_dict.txt](docs/SourceCode/data/basic_dicts/char_dict.txt)
|        |   |      (simple char vocab without indices, one char per line)
|        |   |-[word_dict.txt](docs/SourceCode/data/basic_dicts/word_dict.txt)
|        |          (word vocab with occurrences, for GloVe)
|        |
|        |--shakespeare_corpus
|        |   |--raw
|        |   |   |-[citation-n-legal_stuff.txt](docs/SourceCode/data/shakespeare_corpus/raw/citation-n-legal_stuff.txt)
|        |   |   |      (legal information from Project Gutenberg)
|        |   |   |-[shakespeare_complete.txt](docs/SourceCode/data/shakespeare_corpus/raw/shakespeare_complete.txt)
|        |   |   |      (complete works of Shakespeare)
|        |   |   |-[shakespeare_sonnets.txt](docs/SourceCode/data/shakespeare_corpus/raw/shakespeare_sonnets.txt)
|        |   |          (the sonnets)
|        |   |--tokenized
|        |       |-[shakespeare_tokenized.txt](docs/SourceCode/data/shakespeare_corpus/tokenized/shakespeare_tokenized.txt)
|        |              (complete works of shakespeare tokenized)
|        |--sonnets
|        |   |--raw
|        |   |   |-[sonnets.txt](docs/SourceCode/data/sonnets/raw/sonnets.txt)
|        |   |   |      (all sonnets in a single file)
|        |   |   |-[test.txt](docs/SourceCode/data/sonnets/raw/test.txt)
|        |   |   |      (test set--16 sonnnets)
|        |   |   |-[valid.txt](docs/SourceCode/data/sonnets/raw/valid.txt)
|        |   |          (development/validation set--16 sonnets)
|        |   |--tokenized
|        |       |-[sonnets_tokenized.txt](docs/SourceCode/data/sonnets/tokenized/sonnets_tokenized.txt)
|        |       |      (all sonnets in one file, tokenized)
|        |       |-[train_tokenized.txt](docs/SourceCode/data/sonnets/tokenized/train_tokenized.txt)
|        |       |      (training set, tokenized)
|        |       |-[test_tokenized.txt](docs/SourceCode/data/sonnets/tokenized/test_tokenized.txt)
|        |       |      (test set, tokenized)
|        |       |-[valid_tokenized.txt](docs/SourceCode/data/sonnets/tokenized/valid_tokenized.txt)
|        |              (development set, tokenized)
|        |-[GloVe_vectors_trimmed.200d.npz](docs/SourceCode/data/GloVe_vectors_trimmed.200d.npz)
|               (200-dimensional word embeddings trained on entire corpus,
|                trimmed to only include words found in the sonnet files)
|--tools
    |-[build_dictionary_char.py](docs/tools/build_dictionary_char.py)
    |      (build a char dictionary (in cPickle), by Miyamoto & Cho)
    |-[build_dictionary_word.py](docs/tools/build_dictionary_word.py)
    |      (build a word dictionary (in cPickle), by Miyamoto & Cho)
    |-[tokenize_file.py](docs/tools/tokenize_file.py)
           (preprocess files fed to the model, dictionary scripts, and GloVe)

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
